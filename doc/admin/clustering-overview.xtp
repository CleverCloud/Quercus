<document>
<header>
<title>Clustering Overview</title>
<description>
<p>
Clustering and load-balancing are often thought to be very complex issues. However, 
these issues are often unavoidable as applications grow beyond the hardware 
limitations of a single machine and users demand guaranteed up-time. Resin provides 
very simple but robust facilities for load-balancing, fail-over, clustering, 
elastic (cloud) computing, distributed caching and clustered 
deployment/administration. You can incrementally utilize these features as the needs 
of your application evolves.
</p>
</description>
</header>
<body>
<localtoc/>
<s1 title="Concepts">
<p>The basic concepts behind clustering and load-balancing are relatively simple.</p>
<p>
Most web applications start their life-cycle being deployed to a single server. This 
is a very simple deployment model and certainly the one developers are most familiar 
with since single servers are usually used for development and testing (especially on 
a developer's local machine). The problem is that as the usage of a web application 
grows beyond moderate numbers, the hardware limitations of a single machine simply is 
not enough (the problem is even more acute when you have more than one high-use 
application deployed to a single machine). The physical hardware limits of a server 
usually manifest themselves as chronic high CPU and memory usage despite reasonable 
performance tuning and hardware upgrades.
</p>
<p>
Server <i>load-balancing</i> solves this problem by allowing you to deploy a single 
web application to more than one physical machine. These machines can then be used to 
share the web application traffic, reducing the total workload on a single machine and 
providing better performance from the perspective of the user. We'll discuss Resin's 
load-balancing capabilities in greater detail shortly, but load-balancing is usually 
achieved by transparently redirecting network traffic across multiple machines at the 
systems level via a hardware or software load-balancer (both of which Resin supports). 
Load-balancing also increases the reliability/up-time of a system because even if one 
or more servers go down or are brought down for maintenance, other servers can still 
continue to handle traffic. With a single server application, any down-time is 
directly visible to the user, drastically decreasing reliability.
</p>
<p>
If web applications were entirely stateless, load-balancing alone would be sufficient 
in meeting all application server scalability needs. In reality, most web applications 
are relatively heavily stateful. Even very simple web applications use the HTTP 
session to keep track of current login, shopping-cart-like functionality and so on. 
Component oriented web frameworks like JSF and Wicket in particular tend to heavily 
utilize the HTTP session to achieve greater development abstraction. Maintaining 
application state is also very natural for the CDI (Resin CanDI) and JBoss Seam 
programming models with stateful, conversational components. When web applications are 
load-balanced, application state must somehow be shared across application servers. 
While in most cases you will likely be using a <i>sticky session</i> (discussed in 
detail shortly) to pin a session to a single server, sharing state is still important 
for <i>fail-over</i>. Fail-over is the process of seamlessly transferring over an 
existing session from one server to another when a load-balanced server fails or is 
taken down (probably for upgrade). Without fail-over, the user would lose a session 
when a load-balanced server experiences down-time. In order for fail-over to work, 
application state must be synchronized or replicated in real time across a set of 
load-balanced servers. This state synchronization or replication process is generally 
known as <i>clustering</i>. In a similar vein, the set of synchronized, load-balanced 
servers are collectively called a <i>cluster</i>. Resin supports robust clustering 
including persistent sessions, distributed sessions and dynamically adding/removing 
servers from a cluster (elastic/cloud computing).
</p>
<p>
Based on the infrastructure required to support load-balancing, fail-over and 
clustering, Resin also has support for an EHCache or Coherence style distributed 
caching API, clustered application deployment as well as tools for administering the 
cluster. 
</p>
</s1>
<s1 title="Load Balancing">
<!-- XXX: Should we add some images here? -->
<p>
Clustering and load-balancing are intrinsic to resin.xml configuration - you may only 
define &lt;server> and &lt;host> inside a &lt;cluster> tag. The &lt;cluster> tag 
defines a logically grouped set of servers to be load-balanced and clustered across. 
Although you can definitely use a hardware load balancer with Resin, it is easier to 
understand how Resin clustering works via it's support for software based 
load-balancing. Load balancing in Resin is accomplished through the 
<a href="http-rewrite-ref.xtp">&lt;resin:LoadBalance></a> tag placed on a cluster.
In effect, the <a href="http-rewrite-ref.xtp">&lt;resin:LoadBalance></a> tag turns a 
set of servers in a cluster into a software load-balancer. This makes a lot of sense 
in light of the fact that as the traffic on your application requires multiple 
servers, your site will naturally be split into two tiers: an application server tier 
for running your web applications and a web/HTTP server tier talking to end-point 
browsers, caching static/non-static content, and distributing load across servers in 
the application server tier.
</p>
<p>
The best way to understand how this works is through a simple example configuration. 
The following resin.xml configuration shows servers split into application and web 
tiers with the web tier doing content caching as well as load-balancing:
</p>
<example title="Example: resin.xml for Load-Balancing">
&lt;resin xmlns="http://caucho.com/ns/resin"
          xmlns:resin="urn:java:com.caucho.resin"&gt;

  &lt;cluster-default>
    &lt;resin:import path="${__DIR__}/app-default.xml"/>
  &lt;/cluster-default>

  &lt;cluster id="app-tier"&gt;
    &lt;server id="app-a" address="192.168.0.10" port="6800"/&gt;
    &lt;server id="app-b" address="192.168.0.11" port="6800"/&gt;

    &lt;host id=""&gt;
      &lt;web-app id="" root-directory="/var/www/htdocs"/>
    &lt;/host&gt;
  &lt;/cluster&gt;

  &lt;cluster id="web-tier"&gt;
    &lt;server id="web-a" address="192.168.0.1" port="6800"&gt;
      &lt;http port="80"/&gt;
    &lt;/server>

    &lt;cache memory-size="256M"/&gt;

    &lt;host id=""&gt;
    
      &lt;resin:LoadBalance regexp="" cluster="app-tier"/>

    &lt;/host&gt;
  &lt;/cluster&gt;

&lt;/resin&gt;
</example>
<p>
In the configuration above, the <var>web-tier</var> cluster server load balances 
across the <var>app-tier</var> cluster servers because of the 
<var>cluster</var> attribute specified in the &lt;resin:LoadBalance> tag. 
The &lt;cache> tag enables proxy caching at the web tier. The web-tier forwards 
requests evenly and skips any app-tier server that's down for maintenance/upgrade or 
restarting due to a crash. The load balancer also steers traffic from a single session 
to the same app-tier server (a.k.a sticky sessions), improving caching and session 
performance.
</p>
<p>
Each app-tier server produces the same application because they have the same virtual 
hosts, web-applications and Servlets, and use the same resin.xml.  Adding a new
machine just requires adding a new &lt;server> tag to the cluster. Each server has a 
unique name like "app-b" and a TCP cluster-port consisting of an &lt;IP,port>, 
so the other servers can communicate with it. Although you can start multiple Resin 
servers on the same machine, TCP requires the &lt;IP,port> to be unique, so you might 
need to assign unique ports like 6801, 6802 for servers on the same machine. On 
different machines, you'll use unique IP addresses. Because the cluster-port is for 
Resin servers to communicate with each other, they'll typically be private IP 
addresses like 192.168.1.10 and not public IP addresses. In particular, the load 
balancer on the web-tier uses the cluster-port of the app-tier to forward HTTP 
requests.
</p>
<p>
All three servers will use the same resin.xml, which makes managing multiple server
configurations pretty easy. The servers are named by the server <var>id</var> 
attribute, which must be unique, just as the &lt;IP,port>. When you start
a Resin instance, you'll use the server-id as part of the command line:
</p>
<example title="Starting Servers">
192.168.0.10> java -jar lib/resin.jar -server app-a start
192.168.0.11> java -jar lib/resin.jar -server app-b start
192.168.0.1> java -jar lib/resin.jar -server web-a start
</example>
<p>
Since Resin lets you start multiple servers on the same machine, a small site might 
start the web-tier server and one of the app-tier servers on one machine, and start 
the second server on a second machine. You can even start all three servers on the 
same machine, increasing reliability and easing maintenance, without addressing the 
additional load (although it will still be problematic if the physical machine itself 
and not just the JVM crashes). If you do put multiple servers on the same machine, 
remember to change the <var>port</var> to something like 6801, etc so the TCP binds do 
not conflict.
</p>
<p>
In the <a href="resin-admin.xtp">/resin-admin</a> management page, you can manage all 
three servers at once, gathering statistics/load and watching for any errors.  When 
setting up /resin-admin on a web-tier server, you'll want to remember to add a separate 
&lt;web-app> for resin-admin to make sure the &lt;rewrite-dispatch> doesn't 
inadvertantly send the management request to the app-tier.
</p>
<s2 title="Sticky/Persistent Sessions">
<p>
To understand what sticky sessions are and why they are important, it is easiest to
see what will happen without them. Let us take our previous example and assume that 
the web tier distributes sessions across the application tier in a totally random 
fashion. Recall that while Resin can replicate the HTTP session across the cluster, it 
does not do so by default. Now lets assume that the first request to the web 
application resolves to app-a and results in the login being stored in the session. 
If the load-balancer distributes sessions randomly, there is no guarantee that the 
next request will come to app-a. If it goes to app-b, that server instance will have 
no knowledge of the login and will start a fresh session, forcing the user to login 
again. The same issue would be repeated as the user continues to use the application 
from in what their view should be a single session with well-defined state, making for 
a pretty confusing experience with application state randomly getting lost!
</p>
<p>
One way to solve this issue would be to fully synchronize the session across the 
load-balanced servers. Resin does support this through it's clustering features 
(which is discussed in detail in the following sections). The problem is that the cost 
of continuously doing this synchronization across the entire cluster is very high and 
relatively unnecessary. This is where sticky sessions come in. With sticky sessions, 
the load-balancer makes sure that once a session is started, any subsequent requests 
from the client go to the same server where the session resides. By default, the Resin 
load-balancer enforces sticky sessions, so you don't really need to do anything to 
enable it.
</p>
<p>
Resin accomplishes this by encoding the session cookie with the host name that started 
the session. Using our example, the hosts would generate cookies like this:
</p>
<deftable>
<tr>
  <th>Index</th>
  <th>Cookie Prefix</th>
</tr>
<tr>
  <td>1</td>
  <td><var>a</var>xxx</td>
</tr>
<tr>
  <td>2</td>
  <td><var>b</var>xxx</td>
</tr>
</deftable>
<p>On the web-tier, Resin will decode the session cookie and send it to the appropriate 
host. So <var>bacX8ZwooOz</var> would go to app-b. In the infrequent case that app-b 
fails, Resin will send the request to app-a. Because the session is not clustered, the 
user will lose the session but they won't see a connection failure (to see how
to avoid losing the session, check out the following section on clustering).
</p>
</s2>
<s2 title="Manually Choosing a Server">
<p>
For testing purposes you might want to send requests to a specific servers in the 
app-tier manually. You can easily do this since the web-tier uses the value of the 
<var>jsessionid</var> to maintain sticky sessions. You can include an explicit 
<var>jsessionid</var> to force the web-tier to use a particular server in the app-tier.
</p>
<p>
Since Resin uses the first character of the <var>jsessionid</var> to identify the 
server to use, starting with 'a' will resolve the request to app-a. 
If wwww.example.com resolves to your web-tier, then you can use values like this for 
testing:
</p>
<ol>
<li>http://www.example.com/proxooladmin;jsessionid=abc</li>
<li>http://www.example.com/proxooladmin;jsessionid=bcd</li>
<li>http://www.example.com/proxooladmin;jsessionid=cde</li>
<li>http://www.example.com/proxooladmin;jsessionid=def</li>
<li>http://www.example.com/proxooladmin;jsessionid=efg</li>
<li>etc.</li>
</ol>
<p>
You can also use this fact to configure an external sticky load-balancer (likely a 
high-performance hardware load-balancer) and eliminate the web tier altogether. In 
this case, this is how the Resin configuration might look like:
</p>
<example title="resin.xml with Hardware Load-Balancer">
&lt;resin xmlns="http://caucho.com/ns/resin"&gt;
  &lt;cluster id="app-tier"&gt;
    &lt;server-default>
      &lt;http port='80'/&gt;
    &lt;/server-default>

    &lt;server id='app-a' address='192.168.0.1'/&gt;
    &lt;server id='app-b' address='192.168.0.2'/&gt;
    &lt;server id='app-c' address='192.168.0.3'/&gt;

  &lt;/cluster&gt;
&lt;/resin&gt;
</example> 
<p>
Each server will be started as <var>-server a</var>, <var>-server b</var>, etc to grab 
its specific configuration.
</p>
</s2>
<s2 title="Socket Pooling, Timeouts, and Failover">
<p>For efficiency, Resin's load balancer manages a pool of sockets connecting to the 
app-tier servers. If Resin forwards a new request to an app-tier server and it has an 
idle socket available in the pool, it will reuse that socket, improving performance 
and minimizing network load. Resin uses a set of timeout values to manage the socket
pool and to handle any failures or freezes of the backend servers. The following 
diagram illustrates the main timeout values:</p>
<figure src="load-balance-idle-time.png"/>
<ul>
<li><b>load-balance-connect-timeout</b>: the load balancer timeout
for the <code>connect()</code> system call to complete to
the app-tier (5s).</li>
<li><b>load-balance-idle-time</b>: load balancer timeout
for an idle socket before closing it automatically (5s).</li>
<li><b>load-balance-recover-time</b>: the load balancer connection failure wait
time before trying a new connection (15s).</li>
<li><b>load-balance-socket-timeout</b>: the load balancer
timeout for a valid request to complete (665s).</li>
<li><b>keepalive-timeout</b>: the app-tier timeout for a keepalive
connection (15s)</li>
<li><b>socket-timeout</b>: the app-tier timeout for a read or
write (65s)</li>
</ul>
<p>When an app-tier server is down due to maintenance or a crash, Resin will use the 
<b>load-balance-recover-time</b> as a delay before retrying
the downed server.  With the failover and recover timeout, the load balancer
reduces the cost of a failed server to almost no time at all.  Every
recover-time, Resin will try a new connection and wait for
<b>load-balance-connect-timeout</b> for the server to respond.  At most, one
request every 15 seconds might wait an extra 5 seconds to connect to the
backend server.  All other requests will automatically go
to the other servers.</p>
<p>The socket-timeout values tell Resin when a socket connection is
dead and should be dropped.  The web-tier timeout
<b>load-balance-socket-timeout</b> is much larger than the app-tier
timeout <b>socket-timeout</b> because the web-tier needs to wait for
the application to generate the response.  If your application has some
very slow pages, like a complicated nightly report, you may need to
increase the <b>load-balance-socket-timeout</b> to avoid the web-tier
disconnecting it.</p>
<p>Likewise, the <b>load-balance-idle-time</b> and <b>keepalive-timeout</b>
are a matching pair for the socket idle pool.  The idle-time tells the
web-tier how long it can keep an idle socket before closing it.  The
keepalive-timeout tells the app-tier how long it should listen for
new requests on the socket.  The <b>keepalive-timeout</b> must be
significantly larger than the <b>load-balance-idle-time</b> so the app-tier
doesn't close its sockets too soon.  The keepalive timeout can be large
since the app-tier can use the
<a href="http-server-ref.xtp">keepalive-select</a>
manager to efficiently wait for many connections at once.</p>
</s2>
<s2 title="Dispatching">
<p>&lt;resin:LoadBalance> is part of Resin's <a href="http-rewrite.xtp">rewrite</a> 
capabilities, Resin's equivalent of the Apache mod_rewrite module, providing powerful
and detailed URL matching and decoding, so more complicated sites might load-balance 
based on the virtual host or URL.</p>
<p>In most cases, the web-tier will dispatch everything to the app-tier servers. 
Because of Resin's
<a href="../doc/proxy-cache.xtp">proxy cache</a>, the web-tier servers
will serve static pages as fast as if they were local pages.</p>
<p>In some cases, though, it may be important to send different
requests to different backend clusters. The
<a href="http://caucho.com/resin-javadoc/com/caucho/rewrite/LoadBalance.html">&lt;resin:LoadBalance></a> 
tag can choose clusters based on URL patterns when such capabilities are needed.</p>
<p>The following <a href="http-rewrite.xtp">rewrite</a> example
keeps all *.png, *.gif, and *.jpg files on the web-tier, sends
everything in /foo/* to the foo-tier cluster, everything in /bar/* to
the bar-tier cluster, and keeps anything else on the web-tier.</p>
<example title="Example: resin.xml Split Dispatching">
&lt;resin xmlns="http://caucho.com/ns/resin"
          xmlns:resin="urn:java:com.caucho.resin">

  &lt;cluster-default>
     &lt;resin:import path="${__DIR__}/app-default.xml"/>
  &lt;/cluster-default>
      
  &lt;cluster id="web-tier">
    &lt;server id="web-a">
      &lt;http port="80"/>
    &lt;/server>

    &lt;cache memory-size="64m"/>

    &lt;host id="">
      &lt;web-app id="/">

        &lt;resin:Dispatch regexp="(\.png|\.gif|\.jpg)"/>

        &lt;resin:LoadBalance regexp="^/foo" cluster="foo-tier"/>

        &lt;resin:LoadBalance regexp="^/bar" cluster="bar-tier"/>

      &lt;/web-app>
    &lt;/host>
  &lt;/cluster>

  &lt;cluster id="foo-tier">
    ...
  &lt;/cluster>

  &lt;cluster id="bar-tier">
    ...
  &lt;/cluster>
&lt;/resin>
</example>
</s2>
<p>
For details on the tags used for clustering, please refer to 
<a href="clustering-ref.xtp">this page</a>.
</p>
</s1>
<s1 title="Clustering">
<!-- XXX: Should we add some images here? -->
<p>
The most easily understood process of clustering (synchronizing state across a set of
servers) is to replicate the entire session state across the cluster on every change in 
real time. Although this is possible in theory the problem is that this model of 
clustering does not scale very well across a mildly large set of servers and often 
floods internal network traffic with frivolous synchronization requests. Instead of 
opting for this "brute force" approach, Resin opts to do what is sometimes referred 
to as "buddy replication" clustering (most specofically in reference to replication
in <a href="http://www.jboss.org/jbosscache">JBossCache</a>).
</p>
<p>
In this scheme, the session state is stored on a 
single "master" server (usually the server that originated the session). One or more 
other servers are randomly chosen to store a backup copy of the session (the "buddies"). 
In case the server that owns the session goes down, one of the back-up servers take over
as the owner. All other servers get an updated copy of the data as needed from the server
that owns the data. The data is always fully synchronized across the owning and backup
servers. By default, only one server is chosen as the back-up, but you can choose to have
two back-ups for extra reliability (together these three servers are the called the 
"triad" - a concept we will discuss further in the elastic computing section). 
</p>
<p>
This scheme works especially nicely with sticky sessions since only one server needs to 
have a copy of the session data anyway. It also works with simple hardware based 
load-balancers that do not support sticky sessions since any server has access to 
session data, although this approach incurs more network bandwidth usage. Also, while
using sticky sessions, you have the option of not doing clustering at all. While not using
sticky sessions, the only way to ensure session consistency is using clustering.
</p>
<p>
The following is a simple example of how to enable clustering on Resin:
</p>
<example>
&lt;resin xmlns="http://caucho.com/ns/resin"&gt;
  ...
  &lt;cluster id="app-tier"&gt;
    &lt;server id="app-a" host="192.168.0.1" port="6802"/>
    &lt;server id="app-b" host="192.168.0.2" port="6802"/>
    ...
  &lt;/cluster>
&lt;/resin>
</example>
<example>
&lt;web-app xmlns="http://caucho.com/ns/resin"&gt;
  &lt;session-config&gt;
    &lt;use-persistent-store="true"/&gt;
  &lt;/session-config&gt;
&lt;/web-app&gt;
</example>
<p>
The <a href="deploy-ref.xtp#session-config">&lt;use-persistent-store&gt;</a> tag under
<a href="deploy-ref.xtp#session-config">&lt;session-config&gt;</a> is used to enable 
clustering. Note, clustering is enabled on a per-web-application basis since not all
web applications under a host need be clustered. If you want to cluster all web 
applications under a host, you can place &lt;use-persistent-store&gt; under 
<a href="deploy-ref.xtp#web-app-default">&lt;web-app-default&gt;</a>. The following
example shows how you can do this: 
</p>
<example title="Example: Clustering for All Web Applications">
&lt;resin xmlns="http://caucho.com/ns/resin"&gt;
  &lt;cluster&gt;
    &lt;server id="a" address="192.168.0.1" port="6800"/&gt;
    &lt;server id="b" address="192.168.0.2" port="6800"/&gt;

    &lt;persistent-store type="cluster"&gt;
      &lt;init&gt;
        &lt;triplicate&gt;true&lt;/triplicate&gt;
      &lt;/init&gt;
    &lt;/persistent-store&gt;

    &lt;web-app-default&gt;
      &lt;session-config use-persistent-store="true"/&gt;
    &lt;/web-app-default&gt;
  &lt;/cluster&gt;
&lt;/resin&gt;
</example>
<!-- XXX: Is the following accurate? -->
<p>
The above example also shows how you can override the clustering behaviour for Resin. 
By default, Resin uses TCP based replication (called a "cluster" store) and only one
backup server. By specifying <var>triplicate=true</var>, you are indicating that you
would like to use two backup servers (that is, have three copies of the session - in 
"triplicate"). In addition to the <var>triplicate</var> attribute, the 
&lt;persistent-store type="cluster"&gt; has a number of other attributes:
</p>
<!-- XXX: These need much better descriptions. It is not clear what they do -->
<deftable title="cluster store attributes">
<tr>
  <th>Attribute</th>
  <th>Description</th>
  <th>Default</th>
</tr>
<tr>
  <td>always-load</td>
  <td>Always load the value</td>
  <td>false</td>
</tr>
<tr>
  <td>always-save</td>
  <td>Always save the value</td>
  <td>false</td>
</tr>
<tr>
  <td>max-idle-time</td>
  <td>How long idle objects are stored (session-timeout will invalidate
items earlier)</td>
  <td>24h</td>
</tr>
<tr>
  <td>path</td>
  <td>Directory to store the objects</td>
  <td>required</td>
</tr>
<tr>
  <td>save-backup</td>
  <td>Saves backup copies of all distributed objects (3.2.0).</td>
  <td>true</td>
</tr>
<tr>
  <td>triplicate</td>
  <td>Saves three copies of all distributed objects (3.2.0).</td>
  <td>false</td>
</tr>
<tr>
  <td>wait-for-acknowledge</td>
  <td>Requires the sending server to wait for all acks.</td>
  <td>false</td>
</tr>
</deftable>
<def title="cluster schema">
element persistent-store {
  type { "cluster "}

  element init {
    always-load?
    &amp; always-save?
    &amp; max-idle-time?
    &amp; triplicate?
    &amp; wait-for-acknowledge?
  }
}
</def>
<p>
For single-server configurations, the "cluster" store saves session
data on disk, allowing for recovery after system restart. This is basically identical 
to the file persistence feature discussed below.
</p>
<!-- XXX: I'm having trouble justifying why this is needed beyond as a supplement to 
     the cluster storage model -->
<!-- XXX: Does JDBC store still need to be discussed? -->
<s2 title="File Persistence">
<p>
Besides TCP based clustering, you can use file based storage as a very simple 
mechanism for improving reliability. In this model, the session is stored as files in 
the <var>resin-data</var> directory. When the session changes, the updates are written 
to the file. When Resin loads an application, it will detect and load stored sessions 
from the file. The value of this is that the session can be recovered is the server 
suddenly crashes but is rapidly re-started. After the server is back up and running 
again, the user will be returned to the state that they were in before the crash. File 
storage also has the added benefit of allowing Resin to keep session information for a 
large number of sessions. An efficient memory cache keeps the most active sessions in 
memory and the disk holds all of the sessions without requiring large amounts of 
memory. However, it is important to understand that file based session persistence 
does not synchronization across the cluster and is only useful in the scope of a 
single server.
</p>
<!-- XXX: This needs an example -->
<p>&lt;persistent-store type="file"&gt; has the following attributes:</p>
<!-- XXX: These need much better descriptions. It is not clear what they do -->
<deftable title="file store attributes">
<tr>
  <th>Attribute</th>
  <th>Description</th>
  <th>Default</th>
</tr>
<tr>
  <td>always-load</td>
  <td>Always load the value</td>
  <td>false</td>
</tr>
<tr>
  <td>always-save</td>
  <td>Always save the value</td>
  <td>false</td>
</tr>
<tr>
  <td>max-idle-time</td>
  <td>How long idle objects are stored</td>
  <td>24h</td>
</tr>
<tr>
  <td>path</td>
  <td>Directory to store the sessions</td>
  <td>required</td>
</tr>
</deftable>
</s2>
<s2 title="always-save-session">
<p>Resin's distributed sessions need to know when a session has changed in order to 
save/synchronize the new session value. Although Resin does detect when an application 
calls <var>HttpSession.setAttribute</var>, it can't tell if an internal session value 
has changed. The following Counter class shows the issue:</p>
<example title="Counter.java">
package test;

public class Counter implements java.io.Serializable {
  private int _count;

  public int nextCount() { return _count++; }
}
</example>
<p>Assuming a copy of the Counter is saved as a session attribute, Resin doesn't know
if the application has called <var>nextCount</var>. If it can't detect a change, Resin 
will not backup/synchronize the new session, unless <var>always-save-session</var> 
is set on the &lt;session-config&gt;. When <var>always-save-session</var> is true, 
Resin will back up the entire session at the end of every request. Otherwise, a 
session is only changed when a change is detected.</p>
<example>
...
&lt;web-app id="/foo"&gt;
...
&lt;session-config&gt;
  &lt;use-persistent-store/&gt;
  &lt;always-save-session/&gt;
&lt;/session-config&gt;
...
&lt;/web-app&gt;
</example>
</s2>
<s2 title="Serialization">
<p>Resin's distributed sessions relies on Hessian serialization to save and restore 
sessions. Application objects must implement <var>java.io.Serializable</var> for 
distributed sessions to work.</p>
</s2>
<s2 title="No Distributed Locking">
<p>Resin's clustering does not lock sessions for replication. For browser-based 
sessions, only one request will typically execute at a time. Since browser sessions 
have no concurrency, there's really no need for distributed locking. However, it's a 
good idea to be aware of the lack of distributed locking in Resin clustering.</p>
</s2>
<p>
For details on the tags used for clustering, please refer to 
<a href="clustering-ref.xtp">this page</a>.
</p>
</s1>
<s1 title="Elastic/Cloud Computing Support">
<!-- XXX: Should we add some imeges here? -->
<p>
For many organizations, user demand fluctuates significantly over time. For example, 
many eductational or government institutions do not have much user demand throughout 
most of the year but demand increases dramatically during periods before and after a 
deadline (such as admissions deadlines, tax deadlines, etc). Similarly many e-commerce 
sites experience dramatic increases in traffic around the holidays. For these 
situations, there is really no need to maintain a lot of server capacity throughout 
the year. What these organizations need is a flexible system that allows for 
dynamically adding and removing servers from the cluster as traffic fluctuates. This 
allows these organizations to save money by not having a lot of excess capacity in 
their server-farms that is not used most of the time. This is the central idea behind 
elastic computing. It is also a fundamental capability needed for private and public 
clouds that must allocate computing capacity on demand.
</p>
<!-- XXX: Is all of this correct? -->
<p>
Resin has excellent support for elastic computing through the concept of dynamic 
servers and the triad. A dynamic server is a Resin instance that can be added to the 
cluster without being statically configured in resin.xml. Because these servers can 
also be taken off-line at any time, they are never used as a back-up server and never 
own session data. Rather, they retrieve, cache and synchronize session data from the 
"triad" servers. The triad on the other hand, is a set of three or more statically 
configured servers that basically function as the data store for the cluster. If you 
recall the "triplicate" concept from the TCP based clustering configuration section 
above, triplicate copies of the session are automacitally enabled in the dynamic 
server case for increased reliability. For dynamic clustering to work, ideally all the 
statically defined servers should be up and running, but in the least at least one 
statically defined server must be active at any given time. When dynamic servers join 
the cluster, they announce their availability to the statically defined servers and 
start synchronizing state with the triad. While dynamic servers synchronize data with 
the triad in real-time, they also maintain cached copies of frequently used data as an 
optimization. Each triad member keeps track of currently available dynamic servers. 
The dynamic server information is shared and synchronized across the triad so that 
even if a daynamic server is added while a triad member was down, they are made aware
of the dynamic server as soon as the triad member comes back on-line.
</p>
<p>
Adding a dynamic server to a cluster is a simple two-step process:
</p>
<ol>
<li>Register the dynamic server with a triad server.</li>
<li>Start the new dynamic server using the registration in the previous step.</li>
</ol>
<s2 title="Preliminaries">
<p>
Before adding a dynamic server, you must:
</p>
<ul>
<li>Set up and start a cluster with a triad like the following example:
<example title="Example: conf/resin.xml">
&lt;resin xmlns="http://caucho.com/ns/resin">

  &lt;cluster id="app-tier">
    ...
    &lt;server id="triad-a" address="234.56.78.90" port="6800"/>
    &lt;server id="triad-b" address="34.56.78.90" port="6800"/>
    &lt;server id="triad-c" address="45.67.89.12" port="6800"/>
</example>
Note, a "triad" can have more than just three servers.
</li>
<li>Install at least one admin password, usually in 
<var>admin-users.xml</var></li>
<li>Enable the RemoteAdminService for the cluster like this:
<example>
&lt;resin xmlns="http://caucho.com/ns/resin">

  &lt;cluster id="app-tier">
    ...
    &lt;admin:RemoteAdminService xmlns:admin="urn:java:com.caucho.admin"/>
    ...
</example>
</li>
<li>Enable the dynamic servers feature for the cluster like this:
<example>
&lt;resin xmlns="http://caucho.com/ns/resin">

  &lt;cluster id="app-tier">
    ...
    &lt;dynamic-server-enable>true&lt;/dynamic-server-enable>
    ...
</example>
</li>
</ul>
</s2>
<s2 title="Registering A Dynamic Server">
<p>
For the first step of registration, you can use a JMX tool like jconsole or
simply use the Resin administration web console.  We'll show how to do
the latter method here. For registration, you'll specify three values:
</p>
<deftable title="Dynamic server deployment options">
<tr>
  <th>Name</th>
  <th>Description</th>
</tr>
<tr>
  <td>Server id</td>
  <td>Symbolic identifier of the new dynamic server.</td>
</tr>
<tr>
  <td>IP</td>
  <td>The IP address of the new dynamic server. May also be host name.</td>
</tr>
<tr>
  <td>Port</td>
  <td>The server port of the new dynamic server. Usually 6800.</td>
</tr>
</deftable>
<p>
With these three values, browse to the Resin administration application's
"cluster" tab. If you have enabled dynamic servers for your cluster, you 
should see a form allowing you to register the server in the "Cluster Overview"
table.
</p>
<figure src="dynamic-server-add.png"/>
<p>
Once you have entered the values and added the server, it should show up 
in the table as a dead server because we haven't started it yet.  The
dynamic server's registration will be propagated to all the servers in the
cluster.
</p>
<figure src="dynamic-server-added.png"/>
</s2>
<s2 title="Starting a Dynamic Server">
<p>
Now that we've registered the dynamic server, we can start it
and have it join the cluster.  In order for the new server to be
recognized and accepted by the triad, it needs to start with the
same resin.xml that the triad is using, the name of the cluster it is
joining, and the values entered in the registration step.  These can
all be specified on the command line when starting the server:
</p>
<example>
dynamic-server> java -jar $RESIN_HOME/lib/resin.jar -conf /etc/resin/resin.xml \
                     -dynamic-server app-tier:123.45.67.89:6800 start
</example>
<p>
Specifying the configuration file allows the new server to configure
itself using the &lt;server-default> options, to find the triad servers
of the cluster it is joining, and to authenticate using the administration
logins.  This command starts the server, which immediately contacts the
triad to join the cluster. Once it has successfully joined, the "Cluster"
tab of the administration application should look like this:
</p>
<figure src="dynamic-server-started.png"/>
</s2>
</s1>
<s1 title="Clustered Deployment">
<p>
The simplest mechanism for doing deployment in a clustered environment is to copy 
files to each server's deployment directory. The biggest issue with this approach is 
that it is very difficult to synchronize deployment across the cluster
and it often results in some down-time. Resin clustered deployment is designed to 
solve this problem. Resin has ANT and Maven based APIs to deploy to a cluster in an 
automated fashion. These features work much like session clustering in that 
applications are automatically replicated across the cluster. Because Resin cluster 
deployment is based on Git, it also allows for application versioning and staging 
across the cluster.
</p>
<p>
In order to use Resin clustered deployment, you must first enable remote deployment 
on the server, which is disabled by default. You do this using the following Resin 
configuration:
</p>
<example title="Enabling Resin Clustered Deployment">
&lt;resin xmlns="http://caucho.com/ns/resin"
       xmlns:resin="urn:java:com.caucho.resin">
  &lt;cluster id="app-tier">
    &lt;resin:AdminAuthenticator password-digest="none">
      &lt;resin:user name="admin" password="myadminpass"/>
    &lt;/resin:AdminAuthenticator>
    &lt;resin:RemoteAdminService/>
    &lt;resin:DeployService/>
  ...
  &lt;/cluster>
&lt;/resin>
</example>
<p>
In the example above, both the remote admin service and the deployment service is 
enabled. Note, the admin authenticator must be enabled for any remote administration 
and deployment for obvious security reasons. To keep things simple, we used a 
clear-text password above, but you should likely use a password hash instead.
</p>
<p>
Once the cluster is up and running, you can use the Ant snippet below to do a remote 
deployment:
</p>
<example title="Clustered Deployment via Ant">
&lt;project name="test" default="test" basedir=".">
  &lt;property name="resin.home" value="/usr/share/resin"/>
  &lt;target name="test">
    &lt;taskdef name="resin-upload-war"
             classname="com.caucho.ant.ResinUploadWar">
      &lt;classpath>
        &lt;fileset dir="${resin.home}">
          &lt;include name="lib/*.jar"/>
          &lt;include name="plugins/resin-ant.jar"/>
        &lt;/fileset>
      &lt;/classpath>
    &lt;/taskdef>

    &lt;resin-upload-war server="localhost"
                      port="8080"
                      user="admin"
                      password="myadminpass"
                      warFile="foo.war"/>
  &lt;/target>
&lt;/project>
</example>
<p>
After you run the Ant script above, you will see output like this:
</p>
<example>
[resin-upload-war] Deployed foo.war to tag wars/default/default/foo
</example>
<p>
The output exposes a few important details about the underlying remote deployment 
implementation for Resin that you should understand. Remote deployment for Resin uses 
Git under the hood. In case you are not familiar with it, Git is a newish 
version control system similar to Subversion. A great feature of Git is that it is 
really clever about avoiding redundancy and synchronizing data across a network. Under 
the hood, Resin stores deployed files as nodes in Git with tags representing the type 
of file, development stage, virtual host, web application context root name and 
version. The format used is this:
</p>
<example>
type/stage/virtual host/context root[-version]
</example>
<p>
In the example, all web applications are stored under <var>wars</var>, we didn't 
specify a stage or virtual host in the Ant task so <var>default</var> is used, the 
web application root is foo and no version is used since one was not specified. 
This format is key to the versioning and staging featured we will discuss shortly.
</p>
<p>
As soon as your web application is uploaded to the Resin deployment repository, it is 
propagated to all the servers in the cluster - including any dynamic servers that are 
added to the cluster at a later point in time after initial propagation happens.
</p>
<figure src="deployment.png"/>
<p>
Doing remote deployment with the Resin Maven plug-in is very similar. You'll need to 
setup the plug-in like this:
</p>
<example title="Clustered Deployment via Maven">
&lt;project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
         http://maven.apache.org/maven-v4_0_0.xsd">
  &lt;modelVersion>4.0.0&lt;/modelVersion>
  &lt;groupId>com.test&lt;/groupId>
  &lt;artifactId>test&lt;/artifactId>
  &lt;packaging>war&lt;/packaging>
  &lt;version>1.0-SNAPSHOT&lt;/version>
  &lt;name>Test Maven Web Application&lt;/name>
  &lt;url>http://maven.apache.org&lt;/url>
  &lt;dependencies>
  &lt;/dependencies>
  &lt;pluginRepositories>
    &lt;pluginRepository>
      &lt;snapshots>
        &lt;enabled>true&lt;/enabled>
        &lt;updatePolicy>always&lt;/updatePolicy>
        &lt;checksumPolicy>ignore&lt;/checksumPolicy>
      &lt;/snapshots>
      &lt;id>caucho&lt;/id>
      &lt;name>Caucho&lt;/name>
      &lt;url>http://caucho.com/m2-snapshot&lt;/url>
    &lt;/pluginRepository>
  &lt;/pluginRepositories>
  &lt;build>
    &lt;finalName>foo&lt;/finalName>
    &lt;plugins>
      &lt;plugin>
        &lt;groupId>com.caucho&lt;/groupId>
        &lt;artifactId>resin-maven-plugin&lt;/artifactId>
        &lt;version>4.0-SNAPSHOT&lt;/version>
        &lt;configuration>
          &lt;server>127.0.0.1&lt;/server>
          &lt;port>8086&lt;/port>
          &lt;user>admin&lt;/user>
          &lt;password>myadminpass&lt;/password>
        &lt;/configuration>
      &lt;/plugin>
    &lt;/plugins>
  &lt;/build>
&lt;/project>
</example>
<p>
You can then remote deploy your application from the command line using Maven like so:
</p>
<example title="Maven Remote Deployment">
mvn resin:upload-war
</example>
<s2 title="Deployment Versioning">
<p>
In addition to automatically propagating a deployment to all clustered nodes, Resin 
remote deployment also supports versioning. When you upload an application, you can 
specify a version number for it. Resin then determines what the latest version of an 
application is and renders it to application users as they arrive. Any users of the 
older version of the application can continue using the application until they log off 
or their session is timed out. Next time these users access the application they get 
the newer version instead of the older one. Versioning is very useful for rolling-back 
a deployment quickly. In order to do a roll-back, you will need to set the older 
version of the application already stored in the Resin deployment Git repository as 
the active one.
</p>
<p>
The best way to understand how this works is through a simple example. Here is a 
modified version of the previous Ant deployment example:
</p>
<example title="Deployment Versioning via Ant">
&lt;resin-upload-war server="localhost"
                  port="8080"
                  user="admin"
                  password="myadminpass"
                  warFile="foo.war"
                  version="1.0"/>
</example>
<p>
Notice in this case we've explicitly set a version number (in a real life Ant script 
you would of course likely externalize things like versions through properties instead 
of hard-coding them). After you run the Ant script, you should see something like 
this:
</p>
<example>
[resin-upload-war] Deployed foo.war to tag wars/default/default/foo-1.0
[resin-upload-war] Wrote head version tag wars/default/default/foo
</example>
<p>
The Maven equivalent of this command would look like the following where 
resin.version system property should be explictly specified:
</p>
<example title="Application Versioning via Maven">
mvn resin:upload-war -Dresin.version="1.0"
</example>
<p>
This application could be upgraded with the following Ant script:
</p>
<example title="Upgrading Application Version via Ant">
&lt;resin-upload-war server="localhost"
                  port="8080"
                  user="admin"
                  password="myadminpass"
                  warFile="foo.war"
                  version="2.0"/>
</example>
<p>
In this case the application was upgraded to version 2.0, so the head tag in Git 
will be updated to point to the newly uploaded file, as indicated by Ant task output:
</p>
<example>
[resin-upload-war] Deployed foo.war to tag wars/default/default/foo-2.0
[resin-upload-war] Wrote head version tag wars/default/default/foo
</example>
<p>
The Maven version of the application upgrade would be like this:
</p>
<example title="Upgrading Application Version via Maven">
mvn resin:upload-war -Dresin.version="2.0"
</example>
<p>
Keep in mind, although the head revision was updated so that users start to see the 
new version of the application, the older application remains intact in the deployment 
repository. This is extremely powerful if you need to back out the new deployment 
quickly. This can be done by simply copying back the older file to the head tag like 
this:
</p>
<example title="Reverting to Old Application Version via Ant">
&lt;project name="test" default="test" basedir=".">
  &lt;property name="resin.home" value="/usr/share/resin"/>
  &lt;target name="test">
    &lt;taskdef name="resin-copy-tag"
             classname="com.caucho.ant.ResinCopyTag"/>
    ...
    &lt;resin-copy-tag server="localhost"
                    port="8080"
                    user="admin"
                    password="myadminpass"
                    sourceVersion="1.0"
                    sourceContextRoot="foo"
                    contextRoot="foo"/>
  &lt;/target>
&lt;/project>
</example>
<p>
The key here is specifying the source version to be 1.0. <var>resin-copy-tag</var> 
also has a <var>version</var> attribute to specify where the tag is being copied to, 
which was omitted because we want to copy to the default head revision. The output 
from running the Ant task reveals the end result:
</p>
<example>
[resin-copy-tag] Copying wars/default/default/foo-1.0 to wars/default/default/foo
</example>
<p>
Here is the Maven version of the command:
</p>
<example title="Reverting to Old Application Version via Maven">
mvn resin:copy-tag -Dresin.sourceContextRoot='foo' -Dresin.sourceVersion="1.0" -Dresin.contextRoot="foo"
</example>
<p>
If you feel you don't need extra versions of applications in the repository, you can 
always delete them. For example, you can delete the faulty 2.0 deployment like this:
</p>
<example title="Deleting Application Version via Ant">
&lt;project name="test" default="test" basedir=".">
  &lt;property name="resin.home" value="/usr/share/resin"/>
  &lt;target name="test">
    &lt;taskdef name="resin-delete-tag"
             classname="com.caucho.ant.ResinDeleteTag">
      &lt;classpath>
        &lt;fileset dir="${resin.home}">
          &lt;include name="lib/*.jar"/>
          &lt;include name="plugins/resin-ant.jar"/>
        &lt;/fileset>
      &lt;/classpath>
    &lt;/taskdef>
    ...
    &lt;resin-delete-tag server="localhost"
                      port="8080"
                      user="admin"
                      password="myadminpass"
                      version="2.0"
                      contextRoot="foo"/>
  &lt;/target>
&lt;/project>
</example>
<p>
The Maven version is:
</p>
<example title="Deleting Application Version via Ant">
mvn resin:delete-tag -Dresin.version="2.0" -Dresin.contextRoot="foo"
</example>
<p>
You can also always check out what versions of the application are currently 
installed in the Resin deployment repository:
</p>
<example title="Query Application Versions via Ant">
&lt;project name="test" default="test" basedir=".">
  &lt;property name="resin.home" value="/usr/share/resin"/>
  &lt;target name="test">
    &lt;taskdef name="resin-query-tags"
             classname="com.caucho.ant.ResinQueryTags"/>
    ...
    &lt;resin-query-tags server="localhost"
                      port="8080"
                      user="admin"
                      password="myadminpass"
                      contextRoot="foo"/>
  &lt;/target>
&lt;/project>
</example>
<p>The output from the query should look something like this:</p>
<example>
[resin-query-tags] wars/default/default/foo-1.0
[resin-query-tags] wars/default/default/foo-2.0
</example>
<p>
The Maven version of the query looks like this:
</p>
<example title="Query Application Versions via Maven">
mvn resin:query-tags -Dresin.version='.*
</example>
</s2>
<s2 title="Staging Deployments">
<p>
Stages are essentially application versioning applied at the server level. As we 
discussed in the previous sections, you can apply a development stage to each uploaded 
application. On the other hand, each server instance in the cluster can have an 
associated stage. The server only publishes applications that match the stage it is 
currently in. Applications in all other stages are ignored even if it is propagated 
and stored in the repository for the server instance.
</p>
<figure src="staging.png"/>
<p>
Using this feature you can use an actual production cluster server for things like 
beta testing or user acceptance testing. Staging can also simply be used as away to 
do a last minute reality check in the production environment before doing a final 
roll-out. In this technique, you first deploy an application in something other than 
the default stage. This application is propagated to all instances in the cluster. 
This is how you do it using Ant:
</p>
<example title="Deployment Staging Using Ant">
&lt;resin-upload-war server="localhost"
                  port="8080"
                  user="admin"
                  password="myadminpass"
                  warFile="foo.war"
                  stage="preview"/>
</example>
<p>
Notice the explicit specification of the application stage in the Ant task. The Maven 
version of this looks like this:
</p>
<example title="Deployment Staging Using Maven">
mvn resin:upload-war -Dresin.stage="preview"
</example>
<p>
All servers in the default stage will simply ignore this application even though it 
is in the repository. All servers are normally in the default stage when started 
as below:
</p>
<example>
java -jar $RESIN_HOME/lib/resin.jar -server a start
</example>
<p>
In order for a server to actually publish the application above, the server instance 
must be started in the "preview" stage (in general, you would probably want to start 
a preview stage server as a dynamic server). You can do this as below:
</p>
<example>
java -jar $RESIN_HOME/lib/resin.jar -server b -stage preview start
</example>
<p>
You can use this server to do user acceptance testing or simply a last minute check 
(typically you will use IP blocking to isolate this server from normal production 
users). Once you are satisfied with the results, you can change the application 
to the default stage by doing a copy as below:
</p>
<example title="Changing Deployment Stage Using Ant">
&lt;resin-copy-tag server="localhost"
                port="8080"
                user="admin"
                password="myadminpass"
                sourceStage="preview"
                sourceContextRoot="foo"
                stage="default"
                contextRoot="foo"/>
</example>
<p>The following is the output of this command:</p>
<example>
[resin-copy-tag] Copying wars/preview/default/foo to wars/default/default/foo
</example>
<p>The Maven version of this looks like the following:</p>
<example title="Changing Deployment Stage Using Maven">
mvn resin:copy-tag -Dresin.sourceContextRoot='foo' -Dresin.sourceStage="preview" -Dresin.stage="default"
</example>
<p>
Because the application already resides in all clustered instances, switching things 
over to production happens almost instantaneously. You can interplay staging and 
versioning as well by staging new versions before deployment.
</p>
</s2>
<p>
The details for each Ant and Maven based clustered deployment API is outlined 
<a href="clustering-deployment-ref.xtp">here</a>.
</p>
</s1>
<s1 title="Distributed Caching">
<p>
Distributed caching is a very useful technique for reducing database load and 
increasing application performance. Resin provides a very simple JSR-107 JCache 
based distributed caching API. Distributed caching in Resin basically leverages the 
underlying infrastructure used to support load balancing, clustering and session/state 
replication. Note, since Resin clustering is not transactional, Resin distributed 
caching is also not transactional and does not utilize locking.
</p>
<p>
The first step to using Resin distributed caching is to configure a cache. 
You configure a named distributed cache at the web application level like this:
</p>
<!-- XXX: Is this accurate? Why can't you configure a cache at the cluster level? -->
<example title="Configuring Resin Distributed Cache">
&lt;resin xmlns="http://caucho.com/ns/resin"
       xmlns:cluster="urn:java:com.caucho.distcache">
  &lt;cluster id="">
    &lt;host id="">
      &lt;web-app id="">
        &lt;cluster:ClusterCache>
          &lt;cluster:name>test&lt;/cluster:name>
        &lt;/cluster:ClusterCache> 
      &lt;/web-app>
    &lt;/host>
  &lt;/cluster>
&lt;/resin>
</example>
<p>
You may then inject the cache through CDI and use it in your application via the 
JCache API:
</p>
<!-- XXX: Why is the name not being used in injection? -->
<example title="Using Resin Distributed Cache">
import javax.cache.*;
...
@Inject
private Cache cache;
...
if (cache.get("discount_rate") != null) {
  cache.put("discount_rate", discountRate);
}
</example>
</s1>
</body>
</document>